DATA_DIR = "../data/" # путь до папки с данными
RESULTS_DIR = "../results/" # путь до папки с результатами вычислений
EMBEDDING_DIR = "../embeddings/" # путь до папки с эмбеддингами
MODELS_DIR = "../models/" # путь до папки с моделями
MODEL_NAME = "utrobinmv/t5_translate_en_ru_zh_small_1024" # название модели для перевода
DATASET_NAME_HF = "aiana94/polynews-parallel" # название датасета на huggingface
DATASET_NAME_LOC = "polynews-parallel" # название, под которым датасет будет сохранён локально (+ с предобработкой)

MAX_SEQUENCE_LEN = 256 # оптимальное число токенов в документе (если не достаёт — padding, если перебор — truncation), определялось по гистограмме распределения числа токенов в текстах
FP16 = 0 # Использовать ли вычисление в fp16 (1 — да, 0 — нет)

RANDOM_STATE = 42 # число для задания случайности
TEST_SIZE = 0.2 # размер тестовой выборки
TEST_MAX_SAMPLES = 10 # максимальное число тестовых примеров
TRAIN_MAX_SAMPLES = 50 # максимальное число обучающих примеров

EPOCHS = 10 # число эпох обучения
EPOCHS_PATIENCE = 3 # число эпох без изменения наблюдаемой метрики, после которого обучение прекратится
LEARNING_RATE = 0.00001 # learning rate
BATCH_SIZE = 2 # размер батча (число сэмплов, передаваемых в модель одновременно => чем больше значение - тем быстрее обучение, но хуже качество из-за аккумуляции градиентов)
